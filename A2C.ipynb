{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#A2C 1 - instalacje:"
      ],
      "metadata": {
        "id": "WjxKgB9nLmq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12K_kKT8LUrS"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A2C 2 - kod:"
      ],
      "metadata": {
        "id": "P6wpKutALwYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn\n",
        "from gym import Env, spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3 import PPO, A2C\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "8DMkYlrOLy5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A2C 3 - kod:"
      ],
      "metadata": {
        "id": "dyUXv8axMzmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_Model(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, input_size=36, features_dim=36, hidden_size=128, layers_count=3, dropout=0.2, device=\"cuda\"):\n",
        "        super(LSTM_Model, self).__init__(observation_space, features_dim)\n",
        "        self.layers_count = layers_count\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, layers_count, dropout=dropout, batch_first=True).to(device)\n",
        "        self.linear_1 = nn.Linear(hidden_size * observation_space.shape[1], hidden_size).to(device)\n",
        "        self.linear_2 = nn.Linear(hidden_size, features_dim).to(device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.squeeze(x, dim=-3)\n",
        "        x = torch.nan_to_num(x)\n",
        "        x, (_, _) = self.lstm(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.linear_1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jLG6posqMvZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A2C 4 - kod:"
      ],
      "metadata": {
        "id": "sfaAIa9IM6W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def znorm(data):\n",
        "    std, mean = torch.std_mean(data, dim=-2)\n",
        "    data = (data - mean) / (std + 1e-8)\n",
        "    return data\n",
        "\n",
        "def read_file(fname, v, rolling_len, offset):\n",
        "    df = pd.read_excel(fname)\n",
        "    data = df.iloc[offset:offset+rolling_len, 1:].values\n",
        "    if v == \"train\":\n",
        "        data = data[:int(len(data) * 8 / 10)]\n",
        "    elif v == \"test\":\n",
        "        data = data[int(len(data) * 8 / 10):]\n",
        "    else:\n",
        "        pass\n",
        "    return data.shape[1], torch.Tensor(data)\n",
        "\n",
        "class FinanceEnv(Env):\n",
        "    def __init__(\n",
        "        self,\n",
        "        fname,\n",
        "        v,\n",
        "        rolling_len,\n",
        "        offset\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.observation_length = 20\n",
        "        self.sequence_count, self.raw_sequences = read_file(fname, v, rolling_len, offset)\n",
        "\n",
        "        self.total_profit = 0\n",
        "        self.total_profits = []\n",
        "\n",
        "        self.curr_step = 0\n",
        "\n",
        "        first_state = self.read_frame(0)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-100, high=100, shape=(first_state.unsqueeze(0).shape)\n",
        "        )\n",
        "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.sequence_count,))\n",
        "\n",
        "        self.profit_history = []\n",
        "\n",
        "    def read_frame(self, index):\n",
        "        if index + self.observation_length < self.raw_sequences.shape[-2] - 1:\n",
        "            frame = self.raw_sequences[index: index + self.observation_length, :]\n",
        "            return torch.tensor(frame)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def step(self, actions: List[float]):\n",
        "\n",
        "        actions = actions.flatten()\n",
        "        actions = actions / np.abs(actions).sum()\n",
        "        done = False\n",
        "\n",
        "        shift = 1\n",
        "        prices = self.read_frame(self.curr_step + shift)\n",
        "\n",
        "        if prices != None:\n",
        "            prices = prices[-2:, :]\n",
        "            price_change = (prices[-1, :] - prices[-shift-1, :]) / prices[-shift-1, :]\n",
        "            profit = (torch.tensor(actions) * price_change).sum().item()\n",
        "\n",
        "            self.total_profit += profit\n",
        "            self.profit_history += [profit]\n",
        "\n",
        "            reward = profit\n",
        "\n",
        "            self.curr_step += 1\n",
        "\n",
        "            next_state = self.read_frame(self.curr_step).unsqueeze(0)\n",
        "            next_state = znorm(next_state)\n",
        "\n",
        "        elif prices == None:\n",
        "            done = True\n",
        "            self.total_profits.append(self.total_profit)\n",
        "            next_state = np.empty(self.observation_space.shape, dtype=object)\n",
        "            reward = 0\n",
        "\n",
        "        return (\n",
        "            next_state,\n",
        "            reward,\n",
        "            done,\n",
        "            {\n",
        "                \"curr_step\": self.curr_step,\n",
        "                \"rewards\": [reward],\n",
        "                \"wallet\": self.total_profit,\n",
        "            },\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_profit = 0\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.profit_history = []\n",
        "\n",
        "        observation = self.read_frame(0).unsqueeze(0)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        pass\n",
        "\n",
        "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, rolling_len, offset, verbose=1):\n",
        "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.best_mean_reward = -np.inf\n",
        "        self.wallets = []\n",
        "        self.train_wallets = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        num = len(self.training_env.envs[0].total_profits) - len(self.wallets)\n",
        "        if num > 0:\n",
        "            test = FinanceEnv('gdrive/MyDrive/PracaMag/Dane.xlsx', \"test\", rolling_len, offset)\n",
        "            o = test.reset()\n",
        "            d = False\n",
        "            m = None\n",
        "            while not d:\n",
        "                a, _ = model.predict(o, deterministic=True)\n",
        "                o, _, d, m = test.step(a)\n",
        "            for _ in range(num):\n",
        "                self.wallets.append(m[\"wallet\"])\n",
        "            train = FinanceEnv('gdrive/MyDrive/PracaMag/Dane.xlsx', \"train\", rolling_len, offset)\n",
        "            o = train.reset()\n",
        "            d = False\n",
        "            m = None\n",
        "            while not d:\n",
        "                a, _ = model.predict(o, deterministic=True)\n",
        "                o, _, d, m = train.step(a)\n",
        "            for _ in range(num):\n",
        "                self.train_wallets.append(m[\"wallet\"])\n",
        "        return True"
      ],
      "metadata": {
        "id": "UIz83eCKM-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A2C 5 - kod:"
      ],
      "metadata": {
        "id": "WMl5e3XHNGmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = int(40000)\n",
        "rolling_len = 500\n",
        "\n",
        "now = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
        "res_dir = \"gdrive/MyDrive/PracaMag/results/{}_A2C\".format(now)\n",
        "\n",
        "os.mkdir(res_dir)\n",
        "\n",
        "wallets = []\n",
        "naive_wallets = []\n",
        "\n",
        "offset = 0\n",
        "\n",
        "for i in range(6):\n",
        "    env = FinanceEnv('gdrive/MyDrive/PracaMag/Dane.xlsx', \"train\", rolling_len, offset)\n",
        "    model = A2C(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        policy_kwargs={\n",
        "            \"features_extractor_class\": LSTM_Model,\n",
        "            \"features_extractor_kwargs\": {\n",
        "                \"input_size\": env.sequence_count,\n",
        "                \"features_dim\": env.sequence_count,\n",
        "            }\n",
        "        },\n",
        "        learning_rate=2e-5,\n",
        "        gamma=0.0,\n",
        "        verbose=0\n",
        "    )\n",
        "    callback = SaveOnBestTrainingRewardCallback(500, rolling_len, offset)\n",
        "    model.learn(total_timesteps=epochs, callback=callback)\n",
        "\n",
        "    env = None\n",
        "    test_env = FinanceEnv('gdrive/MyDrive/PracaMag/Dane.xlsx', \"test\", rolling_len, offset)\n",
        "\n",
        "    obs = test_env.reset()\n",
        "    wallet = []\n",
        "    actions = []\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = test_env.step(action)\n",
        "        wallet += [info[\"wallet\"]]\n",
        "        actions += [action]\n",
        "\n",
        "    naive_score = 0\n",
        "    done = False\n",
        "    obs = test_env.reset()\n",
        "    while not done:\n",
        "        action = torch.full((test_env.sequence_count,), 1 / test_env.sequence_count)\n",
        "        obs, reward, done, info = test_env.step(action)\n",
        "        naive_score = info[\"wallet\"]\n",
        "        actions += [action]\n",
        "\n",
        "    naive_wallets.append(naive_score)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    plt.title(label=\"Final wallet through training\")\n",
        "    plt.xlabel('No of runs through data set')\n",
        "    plt.ylabel('Final wallet')\n",
        "    plt.plot(callback.wallets, label=\"test\")\n",
        "    plt.plot(callback.train_wallets, label=\"train\")\n",
        "    plt.legend(loc='upper center')\n",
        "    fig.savefig(\"{}/{}_a\".format(res_dir, i))\n",
        "    plt.close()\n",
        "    fig = plt.figure()\n",
        "    plt.title(label=\"Test wallet through days\")\n",
        "    plt.ylabel(\"Final wallet\")\n",
        "    plt.xlabel('day of trading')\n",
        "    plt.plot(wallet)\n",
        "    fig.savefig(\"{}/{}_b\".format(res_dir, i))\n",
        "    plt.close()\n",
        "\n",
        "    wallets.append(wallet[-1])\n",
        "    offset += 200\n",
        "    test_env = None\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.title(label=\"Final wallet\")\n",
        "plt.plot(list(range(1, len(naive_wallets)+1)), wallets, label=\"rl\")\n",
        "plt.plot(list(range(1, len(naive_wallets)+1)), naive_wallets, label=\"naive\")\n",
        "plt.legend()\n",
        "fig.savefig(\"{}/wallet\".format(res_dir))\n",
        "plt.close()\n",
        "np.savetxt(\"{}/walletarr\".format(res_dir), wallets)\n",
        "np.savetxt(\"{}/naive_walletarr\".format(res_dir), naive_wallets)"
      ],
      "metadata": {
        "id": "qLNktaDDNLvV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}